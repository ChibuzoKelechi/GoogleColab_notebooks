{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport cv2\nimport PIL\nimport pathlib\n\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split  \nfrom tensorflow import keras as tfkeras\nfrom tensorflow.keras import layers, models, callbacks, preprocessing, applications\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.applications import VGG16","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-24T09:44:49.833956Z","iopub.execute_input":"2024-02-24T09:44:49.834341Z","iopub.status.idle":"2024-02-24T09:44:49.841437Z","shell.execute_reply.started":"2024-02-24T09:44:49.834309Z","shell.execute_reply":"2024-02-24T09:44:49.840188Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data_path = '/kaggle/input/rice-image-dataset/Rice_Image_Dataset'\ndata_dir = pathlib.Path(data_path)\n\nwidth=224\nheight=224\n\nprint(data_dir)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:44:49.845669Z","iopub.execute_input":"2024-02-24T09:44:49.846483Z","iopub.status.idle":"2024-02-24T09:44:49.852877Z","shell.execute_reply.started":"2024-02-24T09:44:49.846456Z","shell.execute_reply":"2024-02-24T09:44:49.851968Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"/kaggle/input/rice-image-dataset/Rice_Image_Dataset\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 32\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(height, width),\n  batch_size=batch_size\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(height, width),\n  batch_size=batch_size\n) \n\n# print(train_ds.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:44:49.882333Z","iopub.execute_input":"2024-02-24T09:44:49.883331Z","iopub.status.idle":"2024-02-24T09:46:22.574464Z","shell.execute_reply.started":"2024-02-24T09:44:49.883296Z","shell.execute_reply":"2024-02-24T09:46:22.573404Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Found 75000 files belonging to 5 classes.\nUsing 60000 files for training.\nFound 75000 files belonging to 5 classes.\nUsing 15000 files for validation.\n","output_type":"stream"}]},{"cell_type":"code","source":"# pretrained_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# for layer in pretrained_vgg16.layers:\n#     layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:46:22.576242Z","iopub.execute_input":"2024-02-24T09:46:22.576537Z","iopub.status.idle":"2024-02-24T09:46:22.580539Z","shell.execute_reply.started":"2024-02-24T09:46:22.576513Z","shell.execute_reply":"2024-02-24T09:46:22.579649Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"channels = 3\n\nimage_model = models.Sequential([\n    preprocessing.RandomFlip('horizontal'), # flip left-to-right\n    preprocessing.RandomContrast(0.5),\n    pretrained_vgg16,\n    layers.Conv2D(filters=32, kernel_size=5, activation=\"relu\", padding='same'),\n                  # give the input dimensions in the first layer\n                  # [height, width, color channels(RGB)]\n#     layers.MaxPool2D(),\n    layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding='same'),\n#     layers.MaxPool2D(),\n    layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n    layers.Dropout(0.2),\n    # Classifier Head\n    layers.Flatten(),\n    layers.Dense(units=6, activation=\"relu\"),\n    layers.Dense(5, activation='softmax')\n]) \n\n# image_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-24T10:05:40.301208Z","iopub.execute_input":"2024-02-24T10:05:40.301623Z","iopub.status.idle":"2024-02-24T10:05:40.330827Z","shell.execute_reply.started":"2024-02-24T10:05:40.301594Z","shell.execute_reply":"2024-02-24T10:05:40.329704Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"image_model.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T10:05:43.377856Z","iopub.execute_input":"2024-02-24T10:05:43.378550Z","iopub.status.idle":"2024-02-24T10:05:43.390578Z","shell.execute_reply.started":"2024-02-24T10:05:43.378517Z","shell.execute_reply":"2024-02-24T10:05:43.389620Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Training\nearlystop = tfkeras.callbacks.EarlyStopping(\n    patience=5,\n    verbose=1\n)\n\nhistory = image_model.fit(\n    train_ds, \n    validation_data=val_ds,\n    epochs=20,\n    callbacks=[earlystop]\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T10:14:25.532412Z","iopub.execute_input":"2024-02-24T10:14:25.533101Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/20\n1875/1875 [==============================] - 185s 99ms/step - loss: 0.0715 - accuracy: 0.9809 - val_loss: 0.0601 - val_accuracy: 0.9860\nEpoch 2/20\n1875/1875 [==============================] - 184s 98ms/step - loss: 0.0557 - accuracy: 0.9847 - val_loss: 0.0522 - val_accuracy: 0.9887\nEpoch 3/20\n1875/1875 [==============================] - 184s 98ms/step - loss: 0.0532 - accuracy: 0.9849 - val_loss: 0.0527 - val_accuracy: 0.9871\nEpoch 4/20\n1875/1875 [==============================] - 185s 98ms/step - loss: 0.0531 - accuracy: 0.9852 - val_loss: 0.0523 - val_accuracy: 0.9867\nEpoch 5/20\n1875/1875 [==============================] - 185s 98ms/step - loss: 0.0463 - accuracy: 0.9867 - val_loss: 0.0806 - val_accuracy: 0.9809\nEpoch 6/20\n  52/1875 [..............................] - ETA: 2:29 - loss: 0.0465 - accuracy: 0.9868","output_type":"stream"}]},{"cell_type":"code","source":"test_acc = model.evaluate(val_ds)\n\nprint(f'Loss: {test_acc[0]}')\nprint(f'Accuracy: {100*(test_acc[1])}%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = {\n    0: 'Arborio rice',\n    1: 'Basmati rice'\n}\nimg_path = \"/kaggle/input/testing-images/julie.jpg\"\nfileImage = Image.open(img_path).convert(\"RGB\").resize([width,height],Image.LANCZOS)\nimage = np.array(fileImage)\nmyimage = image.reshape(1, width,height,3)\n\n# prepare pixel data\nmyimage = myimage.astype('float32')\nmyimage = myimage/255.\n\nplt.figure(figsize = (4,2))\nplt.imshow(image)","metadata":{},"execution_count":null,"outputs":[]}]}